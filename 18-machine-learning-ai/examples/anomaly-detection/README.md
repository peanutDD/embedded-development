# å¼‚å¸¸æ£€æµ‹ç¤ºä¾‹

## æ¦‚è¿°

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ—¶åºæ•°æ®å¼‚å¸¸æ£€æµ‹ã€å›¾åƒå¼‚å¸¸æ£€æµ‹å’Œå¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹ã€‚æ”¯æŒå®æ—¶ç›‘æ§ã€ç¦»çº¿åˆ†æå’Œè‡ªé€‚åº”å­¦ä¹ ã€‚

## åŠŸèƒ½ç‰¹æ€§

- **å¤šç±»å‹å¼‚å¸¸æ£€æµ‹**: æ”¯æŒæ—¶åºæ•°æ®ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ•°æ®ç±»å‹
- **å®æ—¶ç›‘æ§**: å®æ—¶æ•°æ®æµå¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦
- **è‡ªé€‚åº”å­¦ä¹ **: åœ¨çº¿å­¦ä¹ å’Œæ¨¡å‹æ›´æ–°
- **å¤šç§ç®—æ³•**: ç»Ÿè®¡æ–¹æ³•ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç®—æ³•
- **å¯è§†åŒ–åˆ†æ**: ä¸°å¯Œçš„å¼‚å¸¸æ£€æµ‹ç»“æœå¯è§†åŒ–
- **å‘Šè­¦ç³»ç»Ÿ**: å¤šçº§åˆ«å¼‚å¸¸å‘Šè­¦å’Œé€šçŸ¥

## é¡¹ç›®ç»“æ„

```
anomaly-detection/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt             # Pythonä¾èµ–
â”œâ”€â”€ models/                      # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ autoencoder/            # è‡ªç¼–ç å™¨æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ time_series_ae.tflite
â”‚   â”‚   â””â”€â”€ image_ae.onnx
â”‚   â”œâ”€â”€ isolation_forest/       # å­¤ç«‹æ£®æ—æ¨¡å‹
â”‚   â”‚   â””â”€â”€ isolation_forest.pkl
â”‚   â””â”€â”€ one_class_svm/          # å•ç±»SVMæ¨¡å‹
â”‚       â””â”€â”€ one_class_svm.pkl
â”œâ”€â”€ src/                         # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_detector.py        # å¼‚å¸¸æ£€æµ‹å™¨åŸºç±»
â”‚   â”œâ”€â”€ time_series_detector.py # æ—¶åºå¼‚å¸¸æ£€æµ‹
â”‚   â”œâ”€â”€ image_detector.py       # å›¾åƒå¼‚å¸¸æ£€æµ‹
â”‚   â”œâ”€â”€ multimodal_detector.py  # å¤šæ¨¡æ€å¼‚å¸¸æ£€æµ‹
â”‚   â”œâ”€â”€ online_learner.py       # åœ¨çº¿å­¦ä¹ å™¨
â”‚   â”œâ”€â”€ alert_system.py         # å‘Šè­¦ç³»ç»Ÿ
â”‚   â””â”€â”€ utils.py                # å·¥å…·å‡½æ•°
â”œâ”€â”€ examples/                    # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ basic_detection.py      # åŸºç¡€å¼‚å¸¸æ£€æµ‹
â”‚   â”œâ”€â”€ realtime_monitoring.py  # å®æ—¶ç›‘æ§
â”‚   â”œâ”€â”€ industrial_monitoring.py # å·¥ä¸šç›‘æ§
â”‚   â”œâ”€â”€ network_monitoring.py   # ç½‘ç»œç›‘æ§
â”‚   â””â”€â”€ iot_monitoring.py       # IoTè®¾å¤‡ç›‘æ§
â”œâ”€â”€ data/                        # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ time_series/            # æ—¶åºæ•°æ®
â”‚   â”œâ”€â”€ images/                 # å›¾åƒæ•°æ®
â”‚   â”œâ”€â”€ sensor_data/            # ä¼ æ„Ÿå™¨æ•°æ®
â”‚   â””â”€â”€ test_data/              # æµ‹è¯•æ•°æ®
â”œâ”€â”€ configs/                     # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ detection_config.yaml   # æ£€æµ‹é…ç½®
â”‚   â”œâ”€â”€ alert_config.yaml       # å‘Šè­¦é…ç½®
â”‚   â””â”€â”€ model_config.yaml       # æ¨¡å‹é…ç½®
â””â”€â”€ tests/                       # æµ‹è¯•ç”¨ä¾‹
    â”œâ”€â”€ test_detectors.py       # æ£€æµ‹å™¨æµ‹è¯•
    â”œâ”€â”€ test_online_learning.py # åœ¨çº¿å­¦ä¹ æµ‹è¯•
    â””â”€â”€ test_performance.py     # æ€§èƒ½æµ‹è¯•
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
python scripts/download_models.py
```

### 2. åŸºç¡€ä½¿ç”¨

```python
from src.time_series_detector import TimeSeriesAnomalyDetector

# åˆ›å»ºæ—¶åºå¼‚å¸¸æ£€æµ‹å™¨
detector = TimeSeriesAnomalyDetector(
    method='autoencoder',
    model_path='models/autoencoder/time_series_ae.tflite',
    threshold=0.1
)

# è®­ç»ƒæˆ–åŠ è½½æ¨¡å‹
detector.fit(normal_data)

# æ£€æµ‹å¼‚å¸¸
import numpy as np
test_data = np.random.random((100, 10))  # 100ä¸ªæ—¶é—´æ­¥ï¼Œ10ä¸ªç‰¹å¾
anomalies = detector.detect(test_data)

print(f"æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸ç‚¹")
for anomaly in anomalies:
    print(f"æ—¶é—´æ­¥ {anomaly['timestamp']}: å¼‚å¸¸åˆ†æ•° {anomaly['score']:.3f}")
```

### 3. å®æ—¶ç›‘æ§

```bash
python examples/realtime_monitoring.py --config configs/detection_config.yaml
```

### 4. å·¥ä¸šç›‘æ§ç¤ºä¾‹

```bash
python examples/industrial_monitoring.py --sensors temperature,pressure,vibration --threshold 0.05
```

## æ ¸å¿ƒç»„ä»¶

### 1. å¼‚å¸¸æ£€æµ‹å™¨åŸºç±»

```python
import numpy as np
import time
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹"""
    POINT = "point"          # ç‚¹å¼‚å¸¸
    CONTEXTUAL = "contextual"  # ä¸Šä¸‹æ–‡å¼‚å¸¸
    COLLECTIVE = "collective"  # é›†ä½“å¼‚å¸¸

@dataclass
class AnomalyResult:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    timestamp: float
    score: float
    is_anomaly: bool
    anomaly_type: AnomalyType
    confidence: float
    details: Dict[str, Any]

class BaseAnomalyDetector(ABC):
    """å¼‚å¸¸æ£€æµ‹å™¨åŸºç±»"""
    
    def __init__(self, threshold: float = 0.1, window_size: int = 100):
        self.threshold = threshold
        self.window_size = window_size
        self.is_fitted = False
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_samples': 0,
            'anomaly_count': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'processing_times': []
        }
        
        # å†å²æ•°æ®ç¼“å­˜
        self.history_buffer = []
        self.max_history = 1000
    
    @abstractmethod
    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> 'BaseAnomalyDetector':
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict_scores(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹å¼‚å¸¸åˆ†æ•°"""
        pass
    
    def detect(self, X: np.ndarray) -> List[AnomalyResult]:
        """æ£€æµ‹å¼‚å¸¸"""
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        start_time = time.time()
        
        # é¢„æµ‹å¼‚å¸¸åˆ†æ•°
        scores = self.predict_scores(X)
        
        # åˆ¤æ–­å¼‚å¸¸
        anomalies = []
        for i, score in enumerate(scores):
            is_anomaly = score > self.threshold
            
            if is_anomaly:
                anomaly_type = self.classify_anomaly_type(X[i], i, X)
                confidence = min(1.0, score / self.threshold)
                
                anomaly = AnomalyResult(
                    timestamp=time.time() + i * 0.001,  # å‡è®¾1msé—´éš”
                    score=float(score),
                    is_anomaly=True,
                    anomaly_type=anomaly_type,
                    confidence=confidence,
                    details=self.get_anomaly_details(X[i], score)
                )
                
                anomalies.append(anomaly)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = (time.time() - start_time) * 1000
        self.update_stats(len(X), len(anomalies), processing_time)
        
        # æ›´æ–°å†å²ç¼“å­˜
        self.update_history(X, scores)
        
        return anomalies
    
    def classify_anomaly_type(self, sample: np.ndarray, index: int, 
                            context: np.ndarray) -> AnomalyType:
        """åˆ†ç±»å¼‚å¸¸ç±»å‹"""
        # ç®€å•çš„å¼‚å¸¸ç±»å‹åˆ†ç±»é€»è¾‘
        # å®é™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚å®ç°æ›´å¤æ‚çš„åˆ†ç±»
        
        if index == 0 or index == len(context) - 1:
            return AnomalyType.POINT
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡å¼‚å¸¸
        window_start = max(0, index - 5)
        window_end = min(len(context), index + 6)
        window_data = context[window_start:window_end]
        
        if len(window_data) > 1:
            window_std = np.std(window_data, axis=0)
            sample_deviation = np.abs(sample - np.mean(window_data, axis=0))
            
            if np.any(sample_deviation > 2 * window_std):
                return AnomalyType.CONTEXTUAL
        
        return AnomalyType.POINT
    
    def get_anomaly_details(self, sample: np.ndarray, score: float) -> Dict[str, Any]:
        """è·å–å¼‚å¸¸è¯¦ç»†ä¿¡æ¯"""
        return {
            'sample_shape': sample.shape,
            'sample_mean': float(np.mean(sample)),
            'sample_std': float(np.std(sample)),
            'sample_min': float(np.min(sample)),
            'sample_max': float(np.max(sample)),
            'anomaly_score': float(score)
        }
    
    def update_stats(self, sample_count: int, anomaly_count: int, processing_time: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_samples'] += sample_count
        self.stats['anomaly_count'] += anomaly_count
        self.stats['processing_times'].append(processing_time)
        
        # ä¿æŒå¤„ç†æ—¶é—´å†å²åœ¨åˆç†èŒƒå›´å†…
        if len(self.stats['processing_times']) > 1000:
            self.stats['processing_times'] = self.stats['processing_times'][-1000:]
    
    def update_history(self, X: np.ndarray, scores: np.ndarray):
        """æ›´æ–°å†å²æ•°æ®"""
        for i, (sample, score) in enumerate(zip(X, scores)):
            self.history_buffer.append({
                'timestamp': time.time() + i * 0.001,
                'sample': sample.copy(),
                'score': score
            })
        
        # é™åˆ¶å†å²ç¼“å­˜å¤§å°
        if len(self.history_buffer) > self.max_history:
            self.history_buffer = self.history_buffer[-self.max_history:]
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        anomaly_rate = 0.0
        if self.stats['total_samples'] > 0:
            anomaly_rate = self.stats['anomaly_count'] / self.stats['total_samples']
        
        avg_processing_time = 0.0
        if self.stats['processing_times']:
            avg_processing_time = np.mean(self.stats['processing_times'])
        
        return {
            'total_samples': self.stats['total_samples'],
            'anomaly_count': self.stats['anomaly_count'],
            'anomaly_rate': anomaly_rate,
            'false_positives': self.stats['false_positives'],
            'false_negatives': self.stats['false_negatives'],
            'average_processing_time_ms': avg_processing_time,
            'threshold': self.threshold,
            'is_fitted': self.is_fitted
        }
    
    def update_threshold(self, new_threshold: float):
        """æ›´æ–°å¼‚å¸¸é˜ˆå€¼"""
        self.threshold = max(0.0, min(1.0, new_threshold))
    
    def get_recent_history(self, n: int = 100) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„å†å²æ•°æ®"""
        return self.history_buffer[-n:]
```

### 2. æ—¶åºå¼‚å¸¸æ£€æµ‹å™¨

```python
import numpy as np
import tensorflow as tf
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
import joblib

class TimeSeriesAnomalyDetector(BaseAnomalyDetector):
    """æ—¶åºå¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, method: str = 'autoencoder', model_path: str = None, 
                 threshold: float = 0.1, window_size: int = 100):
        super().__init__(threshold, window_size)
        
        self.method = method
        self.model_path = model_path
        self.model = None
        self.scaler = StandardScaler()
        
        # æ–¹æ³•ç‰¹å®šå‚æ•°
        if method == 'isolation_forest':
            self.contamination = 0.1
        elif method == 'one_class_svm':
            self.nu = 0.1
        elif method == 'autoencoder':
            self.reconstruction_threshold = threshold
    
    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> 'TimeSeriesAnomalyDetector':
        """è®­ç»ƒæ—¶åºå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        try:
            # æ•°æ®é¢„å¤„ç†
            X_scaled = self.scaler.fit_transform(X)
            
            if self.method == 'isolation_forest':
                self.model = IsolationForest(
                    contamination=self.contamination,
                    random_state=42,
                    n_jobs=-1
                )
                self.model.fit(X_scaled)
                
            elif self.method == 'one_class_svm':
                self.model = OneClassSVM(
                    nu=self.nu,
                    kernel='rbf',
                    gamma='scale'
                )
                self.model.fit(X_scaled)
                
            elif self.method == 'autoencoder':
                if self.model_path and self.model_path.endswith('.tflite'):
                    # åŠ è½½é¢„è®­ç»ƒçš„TensorFlow Liteæ¨¡å‹
                    self.model = tf.lite.Interpreter(model_path=self.model_path)
                    self.model.allocate_tensors()
                    self.input_details = self.model.get_input_details()
                    self.output_details = self.model.get_output_details()
                else:
                    # åˆ›å»ºç®€å•çš„è‡ªç¼–ç å™¨
                    self.model = self.create_autoencoder(X_scaled.shape[1])
                    self.train_autoencoder(X_scaled)
            
            elif self.method == 'statistical':
                # ç»Ÿè®¡æ–¹æ³•ï¼šåŸºäºZ-score
                self.mean_ = np.mean(X_scaled, axis=0)
                self.std_ = np.std(X_scaled, axis=0)
                self.model = 'statistical'  # æ ‡è®°
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {self.method}")
            
            self.is_fitted = True
            print(f"æ—¶åºå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ (æ–¹æ³•: {self.method})")
            
            return self
            
        except Exception as e:
            print(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def create_autoencoder(self, input_dim: int) -> tf.keras.Model:
        """åˆ›å»ºè‡ªç¼–ç å™¨æ¨¡å‹"""
        # ç¼–ç å™¨
        encoder_input = tf.keras.Input(shape=(input_dim,))
        encoded = tf.keras.layers.Dense(64, activation='relu')(encoder_input)
        encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)
        encoded = tf.keras.layers.Dense(16, activation='relu')(encoded)
        
        # è§£ç å™¨
        decoded = tf.keras.layers.Dense(32, activation='relu')(encoded)
        decoded = tf.keras.layers.Dense(64, activation='relu')(decoded)
        decoded = tf.keras.layers.Dense(input_dim, activation='linear')(decoded)
        
        # è‡ªç¼–ç å™¨
        autoencoder = tf.keras.Model(encoder_input, decoded)
        autoencoder.compile(optimizer='adam', loss='mse')
        
        return autoencoder
    
    def train_autoencoder(self, X: np.ndarray, epochs: int = 100):
        """è®­ç»ƒè‡ªç¼–ç å™¨"""
        self.model.fit(
            X, X,
            epochs=epochs,
            batch_size=32,
            validation_split=0.2,
            verbose=0,
            shuffle=True
        )
    
    def predict_scores(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹å¼‚å¸¸åˆ†æ•°"""
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # æ•°æ®é¢„å¤„ç†
        X_scaled = self.scaler.transform(X)
        
        if self.method == 'isolation_forest':
            # å­¤ç«‹æ£®æ—ï¼šå¼‚å¸¸åˆ†æ•°ä¸ºè´Ÿå€¼ï¼Œè½¬æ¢ä¸ºæ­£å€¼
            scores = -self.model.decision_function(X_scaled)
            # å½’ä¸€åŒ–åˆ°[0,1]
            scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)
            
        elif self.method == 'one_class_svm':
            # å•ç±»SVMï¼šå¼‚å¸¸åˆ†æ•°ä¸ºè´Ÿå€¼ï¼Œè½¬æ¢ä¸ºæ­£å€¼
            scores = -self.model.decision_function(X_scaled)
            # å½’ä¸€åŒ–åˆ°[0,1]
            scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)
            
        elif self.method == 'autoencoder':
            if isinstance(self.model, tf.lite.Interpreter):
                # TensorFlow Liteæ¨ç†
                scores = []
                for sample in X_scaled:
                    sample_input = np.expand_dims(sample, axis=0).astype(np.float32)
                    self.model.set_tensor(self.input_details[0]['index'], sample_input)
                    self.model.invoke()
                    reconstruction = self.model.get_tensor(self.output_details[0]['index'])
                    
                    # è®¡ç®—é‡æ„è¯¯å·®
                    mse = np.mean((sample - reconstruction[0]) ** 2)
                    scores.append(mse)
                
                scores = np.array(scores)
            else:
                # æ ‡å‡†TensorFlowæ¨¡å‹
                reconstructions = self.model.predict(X_scaled, verbose=0)
                # è®¡ç®—é‡æ„è¯¯å·®
                scores = np.mean((X_scaled - reconstructions) ** 2, axis=1)
            
        elif self.method == 'statistical':
            # ç»Ÿè®¡æ–¹æ³•ï¼šZ-score
            z_scores = np.abs((X_scaled - self.mean_) / (self.std_ + 1e-8))
            scores = np.max(z_scores, axis=1)  # å–æœ€å¤§Z-scoreä½œä¸ºå¼‚å¸¸åˆ†æ•°
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {self.method}")
        
        return scores
    
    def detect_online(self, sample: np.ndarray) -> Optional[AnomalyResult]:
        """åœ¨çº¿å¼‚å¸¸æ£€æµ‹"""
        if not self.is_fitted:
            return None
        
        # å•æ ·æœ¬æ£€æµ‹
        sample_2d = sample.reshape(1, -1)
        scores = self.predict_scores(sample_2d)
        score = scores[0]
        
        if score > self.threshold:
            return AnomalyResult(
                timestamp=time.time(),
                score=float(score),
                is_anomaly=True,
                anomaly_type=AnomalyType.POINT,
                confidence=min(1.0, score / self.threshold),
                details=self.get_anomaly_details(sample, score)
            )
        
        return None
    
    def update_model(self, X_new: np.ndarray, learning_rate: float = 0.1):
        """åœ¨çº¿æ›´æ–°æ¨¡å‹"""
        if not self.is_fitted:
            return
        
        try:
            if self.method == 'statistical':
                # æ›´æ–°ç»Ÿè®¡å‚æ•°
                X_new_scaled = self.scaler.transform(X_new)
                
                # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
                self.mean_ = (1 - learning_rate) * self.mean_ + learning_rate * np.mean(X_new_scaled, axis=0)
                self.std_ = (1 - learning_rate) * self.std_ + learning_rate * np.std(X_new_scaled, axis=0)
                
            elif self.method == 'autoencoder' and not isinstance(self.model, tf.lite.Interpreter):
                # åœ¨çº¿è®­ç»ƒè‡ªç¼–ç å™¨
                X_new_scaled = self.scaler.transform(X_new)
                self.model.fit(X_new_scaled, X_new_scaled, epochs=1, verbose=0)
            
            # å…¶ä»–æ–¹æ³•æš‚ä¸æ”¯æŒåœ¨çº¿æ›´æ–°
            
        except Exception as e:
            print(f"æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
    
    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_data = {
                'method': self.method,
                'threshold': self.threshold,
                'scaler': self.scaler,
                'is_fitted': self.is_fitted
            }
            
            if self.method in ['isolation_forest', 'one_class_svm']:
                model_data['model'] = self.model
            elif self.method == 'statistical':
                model_data['mean_'] = self.mean_
                model_data['std_'] = self.std_
            elif self.method == 'autoencoder' and not isinstance(self.model, tf.lite.Interpreter):
                self.model.save(path + '_autoencoder.h5')
                model_data['autoencoder_path'] = path + '_autoencoder.h5'
            
            joblib.dump(model_data, path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
            
        except Exception as e:
            print(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        try:
            model_data = joblib.load(path)
            
            self.method = model_data['method']
            self.threshold = model_data['threshold']
            self.scaler = model_data['scaler']
            self.is_fitted = model_data['is_fitted']
            
            if self.method in ['isolation_forest', 'one_class_svm']:
                self.model = model_data['model']
            elif self.method == 'statistical':
                self.mean_ = model_data['mean_']
                self.std_ = model_data['std_']
            elif self.method == 'autoencoder' and 'autoencoder_path' in model_data:
                self.model = tf.keras.models.load_model(model_data['autoencoder_path'])
            
            print(f"æ¨¡å‹å·²ä» {path} åŠ è½½")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
```

### 3. å›¾åƒå¼‚å¸¸æ£€æµ‹å™¨

```python
import cv2
import numpy as np
import tensorflow as tf
from typing import List, Dict, Any, Optional, Tuple

class ImageAnomalyDetector(BaseAnomalyDetector):
    """å›¾åƒå¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, method: str = 'autoencoder', model_path: str = None,
                 threshold: float = 0.1, input_size: Tuple[int, int] = (224, 224)):
        super().__init__(threshold)
        
        self.method = method
        self.model_path = model_path
        self.input_size = input_size
        self.model = None
        
        # å›¾åƒé¢„å¤„ç†å‚æ•°
        self.normalize = True
        self.augment = False
    
    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> 'ImageAnomalyDetector':
        """è®­ç»ƒå›¾åƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        try:
            # é¢„å¤„ç†å›¾åƒæ•°æ®
            X_processed = self.preprocess_images(X)
            
            if self.method == 'autoencoder':
                if self.model_path and self.model_path.endswith('.tflite'):
                    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
                    self.model = tf.lite.Interpreter(model_path=self.model_path)
                    self.model.allocate_tensors()
                    self.input_details = self.model.get_input_details()
                    self.output_details = self.model.get_output_details()
                else:
                    # åˆ›å»ºå·ç§¯è‡ªç¼–ç å™¨
                    self.model = self.create_conv_autoencoder()
                    self.train_autoencoder(X_processed)
            
            elif self.method == 'vae':
                # å˜åˆ†è‡ªç¼–ç å™¨
                self.model = self.create_vae()
                self.train_vae(X_processed)
            
            elif self.method == 'statistical':
                # ç»Ÿè®¡æ–¹æ³•ï¼šåŸºäºåƒç´ ç»Ÿè®¡
                self.pixel_mean = np.mean(X_processed, axis=0)
                self.pixel_std = np.std(X_processed, axis=0)
                self.model = 'statistical'
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {self.method}")
            
            self.is_fitted = True
            print(f"å›¾åƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ (æ–¹æ³•: {self.method})")
            
            return self
            
        except Exception as e:
            print(f"å›¾åƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def preprocess_images(self, images: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†å›¾åƒ"""
        processed_images = []
        
        for img in images:
            # è°ƒæ•´å¤§å°
            if len(img.shape) == 3:
                resized = cv2.resize(img, self.input_size)
            else:
                resized = cv2.resize(img, self.input_size)
                resized = np.expand_dims(resized, axis=-1)
            
            # å½’ä¸€åŒ–
            if self.normalize:
                resized = resized.astype(np.float32) / 255.0
            
            processed_images.append(resized)
        
        return np.array(processed_images)
    
    def create_conv_autoencoder(self) -> tf.keras.Model:
        """åˆ›å»ºå·ç§¯è‡ªç¼–ç å™¨"""
        input_shape = (*self.input_size, 3)
        
        # ç¼–ç å™¨
        encoder_input = tf.keras.Input(shape=input_shape)
        
        # ä¸‹é‡‡æ ·
        x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(encoder_input)
        x = tf.keras.layers.MaxPooling2D(2, padding='same')(x)
        x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
        x = tf.keras.layers.MaxPooling2D(2, padding='same')(x)
        x = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(x)
        encoded = tf.keras.layers.MaxPooling2D(2, padding='same')(x)
        
        # è§£ç å™¨
        x = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(encoded)
        x = tf.keras.layers.UpSampling2D(2)(x)
        x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)
        x = tf.keras.layers.UpSampling2D(2)(x)
        x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(x)
        x = tf.keras.layers.UpSampling2D(2)(x)
        decoded = tf.keras.layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)
        
        # è‡ªç¼–ç å™¨
        autoencoder = tf.keras.Model(encoder_input, decoded)
        autoencoder.compile(optimizer='adam', loss='mse')
        
        return autoencoder
    
    def train_autoencoder(self, X: np.ndarray, epochs: int = 50):
        """è®­ç»ƒå·ç§¯è‡ªç¼–ç å™¨"""
        self.model.fit(
            X, X,
            epochs=epochs,
            batch_size=16,
            validation_split=0.2,
            verbose=1,
            shuffle=True
        )
    
    def predict_scores(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹å›¾åƒå¼‚å¸¸åˆ†æ•°"""
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # é¢„å¤„ç†å›¾åƒ
        X_processed = self.preprocess_images(X)
        
        if self.method == 'autoencoder':
            if isinstance(self.model, tf.lite.Interpreter):
                # TensorFlow Liteæ¨ç†
                scores = []
                for img in X_processed:
                    img_input = np.expand_dims(img, axis=0).astype(np.float32)
                    self.model.set_tensor(self.input_details[0]['index'], img_input)
                    self.model.invoke()
                    reconstruction = self.model.get_tensor(self.output_details[0]['index'])
                    
                    # è®¡ç®—é‡æ„è¯¯å·®
                    mse = np.mean((img - reconstruction[0]) ** 2)
                    scores.append(mse)
                
                scores = np.array(scores)
            else:
                # æ ‡å‡†TensorFlowæ¨¡å‹
                reconstructions = self.model.predict(X_processed, verbose=0)
                # è®¡ç®—é‡æ„è¯¯å·®
                scores = np.mean((X_processed - reconstructions) ** 2, axis=(1, 2, 3))
        
        elif self.method == 'statistical':
            # ç»Ÿè®¡æ–¹æ³•
            scores = []
            for img in X_processed:
                # è®¡ç®—åƒç´ çº§Z-score
                z_scores = np.abs((img - self.pixel_mean) / (self.pixel_std + 1e-8))
                # å–å¹³å‡Z-scoreä½œä¸ºå¼‚å¸¸åˆ†æ•°
                score = np.mean(z_scores)
                scores.append(score)
            
            scores = np.array(scores)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {self.method}")
        
        return scores
    
    def detect_regions(self, image: np.ndarray, patch_size: int = 64, 
                      stride: int = 32) -> List[Dict[str, Any]]:
        """æ£€æµ‹å›¾åƒä¸­çš„å¼‚å¸¸åŒºåŸŸ"""
        if not self.is_fitted:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒ")
        
        height, width = image.shape[:2]
        anomaly_regions = []
        
        # æ»‘åŠ¨çª—å£æ£€æµ‹
        for y in range(0, height - patch_size + 1, stride):
            for x in range(0, width - patch_size + 1, stride):
                # æå–å›¾åƒå—
                patch = image[y:y+patch_size, x:x+patch_size]
                
                # æ£€æµ‹å¼‚å¸¸
                patch_array = np.expand_dims(patch, axis=0)
                scores = self.predict_scores(patch_array)
                score = scores[0]
                
                if score > self.threshold:
                    anomaly_regions.append({
                        'bbox': [x, y, patch_size, patch_size],
                        'score': float(score),
                        'confidence': min(1.0, score / self.threshold)
                    })
        
        # éæå¤§å€¼æŠ‘åˆ¶
        if len(anomaly_regions) > 1:
            anomaly_regions = self.apply_nms(anomaly_regions)
        
        return anomaly_regions
    
    def apply_nms(self, regions: List[Dict[str, Any]], 
                  iou_threshold: float = 0.5) -> List[Dict[str, Any]]:
        """åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶"""
        if len(regions) <= 1:
            return regions
        
        # æŒ‰åˆ†æ•°æ’åº
        regions = sorted(regions, key=lambda x: x['score'], reverse=True)
        
        filtered_regions = []
        
        while regions:
            # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„åŒºåŸŸ
            current = regions.pop(0)
            filtered_regions.append(current)
            
            # ç§»é™¤ä¸å½“å‰åŒºåŸŸé‡å åº¦é«˜çš„åŒºåŸŸ
            remaining = []
            for region in regions:
                iou = self.calculate_iou(current['bbox'], region['bbox'])
                if iou < iou_threshold:
                    remaining.append(region)
            
            regions = remaining
        
        return filtered_regions
    
    def calculate_iou(self, bbox1: List[int], bbox2: List[int]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # è®¡ç®—äº¤é›†
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection = (x_right - x_left) * (y_bottom - y_top)
        
        # è®¡ç®—å¹¶é›†
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def visualize_anomalies(self, image: np.ndarray, 
                          anomaly_regions: List[Dict[str, Any]]) -> np.ndarray:
        """å¯è§†åŒ–å¼‚å¸¸åŒºåŸŸ"""
        result_image = image.copy()
        
        for region in anomaly_regions:
            x, y, w, h = region['bbox']
            score = region['score']
            confidence = region['confidence']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            color = (0, 0, 255)  # çº¢è‰²
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)
            
            # ç»˜åˆ¶åˆ†æ•°
            label = f"Score: {score:.3f}"
            cv2.putText(result_image, label, (x, y - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        
        return result_image
```

### 4. å®æ—¶ç›‘æ§ç³»ç»Ÿ

```python
import threading
import queue
import time
import json
from typing import Dict, List, Any, Optional
from dataclasses import asdict
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from collections import deque

class RealTimeMonitor:
    """å®æ—¶å¼‚å¸¸ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, detectors: Dict[str, BaseAnomalyDetector], 
                 alert_system: Optional['AlertSystem'] = None):
        self.detectors = detectors
        self.alert_system = alert_system
        
        # æ•°æ®é˜Ÿåˆ—
        self.data_queue = queue.Queue(maxsize=1000)
        self.result_queue = queue.Queue(maxsize=1000)
        
        # ç›‘æ§çŠ¶æ€
        self.is_monitoring = False
        self.monitor_thread = None
        
        # å†å²æ•°æ®
        self.history_data = {name: deque(maxlen=1000) for name in detectors.keys()}
        self.anomaly_history = deque(maxlen=100)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_samples': 0,
            'total_anomalies': 0,
            'detector_stats': {name: detector.get_stats() for name, detector in detectors.items()}
        }
    
    def start_monitoring(self):
        """å¯åŠ¨å®æ—¶ç›‘æ§"""
        if self.is_monitoring:
            print("ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        print("å®æ—¶ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_monitoring = False
        
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        
        print("å®æ—¶ç›‘æ§å·²åœæ­¢")
    
    def add_data(self, detector_name: str, data: np.ndarray, metadata: Dict[str, Any] = None):
        """æ·»åŠ ç›‘æ§æ•°æ®"""
        if not self.is_monitoring:
            return
        
        try:
            data_item = {
                'detector_name': detector_name,
                'data': data,
                'timestamp': time.time(),
                'metadata': metadata or {}
            }
            
            self.data_queue.put(data_item, timeout=1)
            
        except queue.Full:
            print("æ•°æ®é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæ•°æ®")
    
    def _monitor_loop(self):
        """ç›‘æ§ä¸»å¾ªç¯"""
        while self.is_monitoring:
            try:
                # è·å–æ•°æ®
                data_item = self.data_queue.get(timeout=1)
                
                # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
                self._process_data_item(data_item)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"ç›‘æ§å¤„ç†é”™è¯¯: {e}")
    
    def _process_data_item(self, data_item: Dict[str, Any]):
        """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
        detector_name = data_item['detector_name']
        data = data_item['data']
        timestamp = data_item['timestamp']
        metadata = data_item['metadata']
        
        if detector_name not in self.detectors:
            print(f"æœªçŸ¥çš„æ£€æµ‹å™¨: {detector_name}")
            return
        
        detector = self.detectors[detector_name]
        
        try:
            # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
            anomalies = detector.detect(data)
            
            # æ›´æ–°å†å²æ•°æ®
            self.history_data[detector_name].append({
                'timestamp': timestamp,
                'data_shape': data.shape,
                'anomaly_count': len(anomalies),
                'metadata': metadata
            })
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            if anomalies:
                self._handle_anomalies(detector_name, anomalies, metadata)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['total_samples'] += len(data)
            self.stats['total_anomalies'] += len(anomalies)
            self.stats['detector_stats'][detector_name] = detector.get_stats()
            
            # å°†ç»“æœæ”¾å…¥ç»“æœé˜Ÿåˆ—
            result_item = {
                'detector_name': detector_name,
                'timestamp': timestamp,
                'anomalies': [asdict(anomaly) for anomaly in anomalies],
                'metadata': metadata
            }
            
            try:
                self.result_queue.put(result_item, timeout=0.1)
            except queue.Full:
                # å¦‚æœç»“æœé˜Ÿåˆ—æ»¡äº†ï¼Œç§»é™¤æœ€æ—§çš„ç»“æœ
                try:
                    self.result_queue.get_nowait()
                    self.result_queue.put(result_item, timeout=0.1)
                except queue.Empty:
                    pass
            
        except Exception as e:
            print(f"æ•°æ®å¤„ç†é”™è¯¯ ({detector_name}): {e}")
    
    def _handle_anomalies(self, detector_name: str, anomalies: List[AnomalyResult], 
                         metadata: Dict[str, Any]):
        """å¤„ç†æ£€æµ‹åˆ°çš„å¼‚å¸¸"""
        for anomaly in anomalies:
            # è®°å½•å¼‚å¸¸å†å²
            anomaly_record = {
                'detector_name': detector_name,
                'timestamp': anomaly.timestamp,
                'score': anomaly.score,
                'anomaly_type': anomaly.anomaly_type.value,
                'confidence': anomaly.confidence,
                'metadata': metadata
            }
            
            self.anomaly_history.append(anomaly_record)
            
            # å‘é€å‘Šè­¦
            if self.alert_system:
                self.alert_system.send_alert(detector_name, anomaly, metadata)
            
            # æ‰“å°å¼‚å¸¸ä¿¡æ¯
            print(f"ğŸš¨ å¼‚å¸¸æ£€æµ‹ [{detector_name}]: "
                  f"åˆ†æ•°={anomaly.score:.3f}, "
                  f"ç±»å‹={anomaly.anomaly_type.value}, "
                  f"ç½®ä¿¡åº¦={anomaly.confidence:.3f}")
    
    def get_recent_results(self, n: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„æ£€æµ‹ç»“æœ"""
        results = []
        temp_queue = queue.Queue()
        
        # ä»ç»“æœé˜Ÿåˆ—ä¸­è·å–æ•°æ®
        while not self.result_queue.empty() and len(results) < n:
            try:
                result = self.result_queue.get_nowait()
                results.append(result)
                temp_queue.put(result)
            except queue.Empty:
                break
        
        # å°†æ•°æ®æ”¾å›é˜Ÿåˆ—
        while not temp_queue.empty():
            self.result_queue.put(temp_queue.get())
        
        return sorted(results, key=lambda x: x['timestamp'], reverse=True)[:n]
    
    def get_anomaly_summary(self, time_window: float = 3600) -> Dict[str, Any]:
        """è·å–å¼‚å¸¸æ‘˜è¦ï¼ˆé»˜è®¤1å°æ—¶çª—å£ï¼‰"""
        current_time = time.time()
        cutoff_time = current_time - time_window
        
        # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å¼‚å¸¸
        recent_anomalies = [
            anomaly for anomaly in self.anomaly_history 
            if anomaly['timestamp'] >= cutoff_time
        ]
        
        # æŒ‰æ£€æµ‹å™¨åˆ†ç»„ç»Ÿè®¡
        detector_summary = {}
        for detector_name in self.detectors.keys():
            detector_anomalies = [
                anomaly for anomaly in recent_anomalies 
                if anomaly['detector_name'] == detector_name
            ]
            
            if detector_anomalies:
                scores = [anomaly['score'] for anomaly in detector_anomalies]
                detector_summary[detector_name] = {
                    'count': len(detector_anomalies),
                    'avg_score': np.mean(scores),
                    'max_score': np.max(scores),
                    'min_score': np.min(scores)
                }
            else:
                detector_summary[detector_name] = {
                    'count': 0,
                    'avg_score': 0.0,
                    'max_score': 0.0,
                    'min_score': 0.0
                }
        
        return {
            'time_window_hours': time_window / 3600,
            'total_anomalies': len(recent_anomalies),
            'detector_summary': detector_summary,
            'anomaly_rate': len(recent_anomalies) / max(1, self.stats['total_samples']) * 100
        }
    
    def export_data(self, filepath: str, format: str = 'json'):
        """å¯¼å‡ºç›‘æ§æ•°æ®"""
        try:
            export_data = {
                'timestamp': time.time(),
                'stats': self.stats,
                'anomaly_history': list(self.anomaly_history),
                'detector_configs': {
                    name: {
                        'threshold': detector.threshold,
                        'method': getattr(detector, 'method', 'unknown'),
                        'stats': detector.get_stats()
                    }
                    for name, detector in self.detectors.items()
                }
            }
            
            if format == 'json':
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ç›‘æ§æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")
            
        except Exception as e:
            print(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
    
    def create_dashboard(self):
        """åˆ›å»ºå®æ—¶ç›‘æ§ä»ªè¡¨æ¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('å®æ—¶å¼‚å¸¸ç›‘æ§ä»ªè¡¨æ¿')
        
        # å¼‚å¸¸æ•°é‡è¶‹åŠ¿
        ax1 = axes[0, 0]
        ax1.set_title('å¼‚å¸¸æ•°é‡è¶‹åŠ¿')
        ax1.set_xlabel('æ—¶é—´')
        ax1.set_ylabel('å¼‚å¸¸æ•°é‡')
        
        # å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ
        ax2 = axes[0, 1]
        ax2.set_title('å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ')
        ax2.set_xlabel('å¼‚å¸¸åˆ†æ•°')
        ax2.set_ylabel('é¢‘æ¬¡')
        
        # æ£€æµ‹å™¨æ€§èƒ½
        ax3 = axes[1, 0]
        ax3.set_title('æ£€æµ‹å™¨æ€§èƒ½')
        ax3.set_xlabel('æ£€æµ‹å™¨')
        ax3.set_ylabel('å¼‚å¸¸ç‡ (%)')
        
        # ç³»ç»ŸçŠ¶æ€
        ax4 = axes[1, 1]
        ax4.set_title('ç³»ç»ŸçŠ¶æ€')
        ax4.axis('off')
        
        def update_dashboard(frame):
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in axes.flat:
                ax.clear()
            
            # é‡æ–°è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
            axes[0, 0].set_title('å¼‚å¸¸æ•°é‡è¶‹åŠ¿')
            axes[0, 1].set_title('å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ')
            axes[1, 0].set_title('æ£€æµ‹å™¨æ€§èƒ½')
            axes[1, 1].set_title('ç³»ç»ŸçŠ¶æ€')
            axes[1, 1].axis('off')
            
            # æ›´æ–°æ•°æ®
            if self.anomaly_history:
                # å¼‚å¸¸æ•°é‡è¶‹åŠ¿
                timestamps = [anomaly['timestamp'] for anomaly in self.anomaly_history]
                if timestamps:
                    axes[0, 0].plot(timestamps, range(1, len(timestamps) + 1))
                
                # å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ
                scores = [anomaly['score'] for anomaly in self.anomaly_history]
                if scores:
                    axes[0, 1].hist(scores, bins=20, alpha=0.7)
                
                # æ£€æµ‹å™¨æ€§èƒ½
                detector_names = list(self.detectors.keys())
                anomaly_rates = []
                
                for name in detector_names:
                    detector_anomalies = [
                        anomaly for anomaly in self.anomaly_history 
                        if anomaly['detector_name'] == name
                    ]
                    rate = len(detector_anomalies) / max(1, len(self.anomaly_history)) * 100
                    anomaly_rates.append(rate)
                
                axes[1, 0].bar(detector_names, anomaly_rates)
                axes[1, 0].tick_params(axis='x', rotation=45)
            
            # ç³»ç»ŸçŠ¶æ€æ–‡æœ¬
            status_text = f"ç›‘æ§çŠ¶æ€: {'è¿è¡Œä¸­' if self.is_monitoring else 'å·²åœæ­¢'}\n"
            status_text += f"æ€»æ ·æœ¬æ•°: {self.stats['total_samples']}\n"
            status_text += f"æ€»å¼‚å¸¸æ•°: {self.stats['total_anomalies']}\n"
            status_text += f"å¼‚å¸¸ç‡: {self.stats['total_anomalies'] / max(1, self.stats['total_samples']) * 100:.2f}%"
            
            axes[1, 1].text(0.1, 0.5, status_text, fontsize=12, verticalalignment='center')
            
            plt.tight_layout()
        
        # åˆ›å»ºåŠ¨ç”»
        ani = animation.FuncAnimation(fig, update_dashboard, interval=2000, cache_frame_data=False)
        
        return fig, ani

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆ›å»ºæ£€æµ‹å™¨
    time_series_detector = TimeSeriesAnomalyDetector(method='isolation_forest')
    image_detector = ImageAnomalyDetector(method='autoencoder')
    
    detectors = {
        'time_series': time_series_detector,
        'image': image_detector
    }
    
    # åˆ›å»ºç›‘æ§ç³»ç»Ÿ
    monitor = RealTimeMonitor(detectors)
    
    # å¯åŠ¨ç›‘æ§
    monitor.start_monitoring()
    
    try:
        # æ¨¡æ‹Ÿæ•°æ®æµ
        for i in range(100):
            # æ—¶åºæ•°æ®
            ts_data = np.random.random((10, 5))
            if i % 20 == 0:  # æ³¨å…¥å¼‚å¸¸
                ts_data += 5
            
            monitor.add_data('time_series', ts_data, {'source': 'sensor_1'})
            
            # å›¾åƒæ•°æ®
            img_data = np.random.random((2, 64, 64, 3))
            monitor.add_data('image', img_data, {'source': 'camera_1'})
            
            time.sleep(0.1)
        
        # è·å–ç»“æœ
        results = monitor.get_recent_results(5)
        summary = monitor.get_anomaly_summary()
        
        print("æœ€è¿‘ç»“æœ:", len(results))
        print("å¼‚å¸¸æ‘˜è¦:", summary)
        
    finally:
        monitor.stop_monitoring()

if __name__ == "__main__":
    main()
```

## åº”ç”¨åœºæ™¯

- **å·¥ä¸šç›‘æ§**: è®¾å¤‡çŠ¶æ€ç›‘æ§å’Œæ•…éšœé¢„è­¦
- **ç½‘ç»œå®‰å…¨**: ç½‘ç»œæµé‡å¼‚å¸¸æ£€æµ‹
- **é‡‘èé£æ§**: äº¤æ˜“å¼‚å¸¸å’Œæ¬ºè¯ˆæ£€æµ‹
- **IoTç›‘æ§**: ç‰©è”ç½‘è®¾å¤‡å¼‚å¸¸ç›‘æ§
- **è´¨é‡æ§åˆ¶**: äº§å“è´¨é‡å¼‚å¸¸æ£€æµ‹
- **ç¯å¢ƒç›‘æµ‹**: ç¯å¢ƒå‚æ•°å¼‚å¸¸å‘Šè­¦

## æ€§èƒ½ä¼˜åŒ–

### 1. ç®—æ³•ä¼˜åŒ–
- **å¢é‡å­¦ä¹ **: æ”¯æŒåœ¨çº¿æ¨¡å‹æ›´æ–°
- **é›†æˆæ–¹æ³•**: å¤šç§ç®—æ³•èåˆæé«˜å‡†ç¡®ç‡
- **è‡ªé€‚åº”é˜ˆå€¼**: åŠ¨æ€è°ƒæ•´å¼‚å¸¸é˜ˆå€¼

### 2. ç³»ç»Ÿä¼˜åŒ–
- **å¹¶è¡Œå¤„ç†**: å¤šçº¿ç¨‹å¼‚å¸¸æ£€æµ‹
- **ç¼“å­˜æœºåˆ¶**: ç¼“å­˜è®¡ç®—ç»“æœ
- **å†…å­˜ç®¡ç†**: ä¼˜åŒ–å†…å­˜ä½¿ç”¨

### 3. éƒ¨ç½²ä¼˜åŒ–
- **æ¨¡å‹å‹ç¼©**: å‡å°‘æ¨¡å‹å¤§å°
- **ç¡¬ä»¶åŠ é€Ÿ**: åˆ©ç”¨GPU/NPUåŠ é€Ÿ
- **è¾¹ç¼˜è®¡ç®—**: æœ¬åœ°åŒ–å¤„ç†å‡å°‘å»¶è¿Ÿ

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¼‚å¸¸æ£€æµ‹ç®—æ³•ï¼Ÿ
A: 
1. æ•°æ®ç±»å‹ï¼šæ—¶åºæ•°æ®ç”¨ç»Ÿè®¡æ–¹æ³•ï¼Œå›¾åƒç”¨æ·±åº¦å­¦ä¹ 
2. æ•°æ®é‡ï¼šå°æ•°æ®é›†ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼Œå¤§æ•°æ®é›†ç”¨æ·±åº¦å­¦ä¹ 
3. å®æ—¶æ€§è¦æ±‚ï¼šå®æ—¶æ£€æµ‹ç”¨è½»é‡çº§ç®—æ³•
4. å‡†ç¡®ç‡è¦æ±‚ï¼šé«˜å‡†ç¡®ç‡ç”¨é›†æˆæ–¹æ³•

### Q: å¦‚ä½•å¤„ç†è¯¯æŠ¥é—®é¢˜ï¼Ÿ
A: 
1. è°ƒæ•´æ£€æµ‹é˜ˆå€¼
2. ä½¿ç”¨å¤šç§ç®—æ³•æŠ•ç¥¨
3. ç»“åˆé¢†åŸŸçŸ¥è¯†è¿‡æ»¤
4. æŒç»­å­¦ä¹ å’Œæ¨¡å‹æ›´æ–°

### Q: å¦‚ä½•è¯„ä¼°æ£€æµ‹æ•ˆæœï¼Ÿ
A: 
1. å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
2. ROCæ›²çº¿å’ŒAUCå€¼
3. è¯¯æŠ¥ç‡å’Œæ¼æŠ¥ç‡
4. å®é™…ä¸šåŠ¡æŒ‡æ ‡

## æ‰©å±•åŠŸèƒ½

- **å¤šæ¨¡æ€èåˆ**: ç»“åˆå¤šç§æ•°æ®ç±»å‹æ£€æµ‹
- **å› æœåˆ†æ**: åˆ†æå¼‚å¸¸äº§ç”ŸåŸå› 
- **é¢„æµ‹æ€§ç»´æŠ¤**: åŸºäºå¼‚å¸¸è¶‹åŠ¿é¢„æµ‹æ•…éšœ
- **è‡ªåŠ¨åŒ–å“åº”**: å¼‚å¸¸è‡ªåŠ¨å¤„ç†å’Œæ¢å¤
- **å¯è§£é‡Šæ€§**: æä¾›å¼‚å¸¸æ£€æµ‹è§£é‡Š

æ¬¢è¿è´¡çŒ®ä»£ç å’Œæå‡ºæ”¹è¿›å»ºè®®ï¼