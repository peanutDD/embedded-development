# è¯­éŸ³è¯†åˆ«ç¤ºä¾‹

## æ¦‚è¿°

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ŒåŒ…æ‹¬éŸ³é¢‘é¢„å¤„ç†ã€ç‰¹å¾æå–ã€è¯­éŸ³è¯†åˆ«å’Œåå¤„ç†çš„å®Œæ•´æµç¨‹ã€‚æ”¯æŒå®æ—¶è¯†åˆ«ã€ç¦»çº¿å¤„ç†å’Œå¤šç§è¯­è¨€æ¨¡å‹ã€‚

## åŠŸèƒ½ç‰¹æ€§

- **å®æ—¶è¯­éŸ³è¯†åˆ«**: æ”¯æŒéº¦å…‹é£å®æ—¶è¯­éŸ³è¾“å…¥è¯†åˆ«
- **ç¦»çº¿å¤„ç†**: å®Œå…¨æœ¬åœ°åŒ–å¤„ç†ï¼Œä¿æŠ¤éšç§
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€
- **å™ªå£°æŠ‘åˆ¶**: å†…ç½®å™ªå£°æŠ‘åˆ¶å’Œå›å£°æ¶ˆé™¤
- **è¯­éŸ³æ´»åŠ¨æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹è¯­éŸ³èµ·æ­¢ç‚¹
- **å…³é”®è¯æ£€æµ‹**: æ”¯æŒç‰¹å®šå…³é”®è¯å”¤é†’å’Œæ£€æµ‹

## é¡¹ç›®ç»“æ„

```
speech-recognition/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt             # Pythonä¾èµ–
â”œâ”€â”€ models/                      # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ acoustic_model/         # å£°å­¦æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ wav2vec2.tflite    # Wav2Vec2æ¨¡å‹
â”‚   â”‚   â””â”€â”€ deepspeech.onnx    # DeepSpeechæ¨¡å‹
â”‚   â”œâ”€â”€ language_model/         # è¯­è¨€æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ chinese_lm.arpa    # ä¸­æ–‡è¯­è¨€æ¨¡å‹
â”‚   â”‚   â””â”€â”€ english_lm.arpa    # è‹±æ–‡è¯­è¨€æ¨¡å‹
â”‚   â””â”€â”€ keyword_spotting/       # å…³é”®è¯æ£€æµ‹
â”‚       â””â”€â”€ keyword_model.tflite
â”œâ”€â”€ src/                         # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_processor.py      # éŸ³é¢‘å¤„ç†å™¨
â”‚   â”œâ”€â”€ feature_extractor.py    # ç‰¹å¾æå–å™¨
â”‚   â”œâ”€â”€ speech_recognizer.py    # è¯­éŸ³è¯†åˆ«å™¨
â”‚   â”œâ”€â”€ language_model.py       # è¯­è¨€æ¨¡å‹
â”‚   â”œâ”€â”€ keyword_spotter.py      # å…³é”®è¯æ£€æµ‹å™¨
â”‚   â””â”€â”€ utils.py                # å·¥å…·å‡½æ•°
â”œâ”€â”€ examples/                    # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ basic_recognition.py    # åŸºç¡€è¯†åˆ«ç¤ºä¾‹
â”‚   â”œâ”€â”€ realtime_recognition.py # å®æ—¶è¯†åˆ«
â”‚   â”œâ”€â”€ file_recognition.py     # æ–‡ä»¶è¯†åˆ«
â”‚   â”œâ”€â”€ keyword_detection.py    # å…³é”®è¯æ£€æµ‹
â”‚   â””â”€â”€ voice_assistant.py      # è¯­éŸ³åŠ©æ‰‹
â”œâ”€â”€ data/                        # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ audio_samples/          # éŸ³é¢‘æ ·æœ¬
â”‚   â”œâ”€â”€ vocabulary/             # è¯æ±‡è¡¨
â”‚   â””â”€â”€ test_recordings/        # æµ‹è¯•å½•éŸ³
â”œâ”€â”€ configs/                     # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ recognition_config.yaml # è¯†åˆ«é…ç½®
â”‚   â”œâ”€â”€ audio_config.yaml       # éŸ³é¢‘é…ç½®
â”‚   â””â”€â”€ model_config.yaml       # æ¨¡å‹é…ç½®
â””â”€â”€ tests/                       # æµ‹è¯•ç”¨ä¾‹
    â”œâ”€â”€ test_audio_processor.py # éŸ³é¢‘å¤„ç†æµ‹è¯•
    â”œâ”€â”€ test_recognizer.py      # è¯†åˆ«å™¨æµ‹è¯•
    â””â”€â”€ test_performance.py     # æ€§èƒ½æµ‹è¯•
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…éŸ³é¢‘å¤„ç†åº“
sudo apt-get install portaudio19-dev  # Linux
# brew install portaudio  # macOS

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
python scripts/download_models.py
```

### 2. åŸºç¡€ä½¿ç”¨

```python
from src.speech_recognizer import EdgeSpeechRecognizer

# åˆ›å»ºè¯­éŸ³è¯†åˆ«å™¨
recognizer = EdgeSpeechRecognizer(
    model_path='models/acoustic_model/wav2vec2.tflite',
    language='zh-cn',
    device='cpu'
)

# åˆå§‹åŒ–è¯†åˆ«å™¨
recognizer.initialize()

# è¯†åˆ«éŸ³é¢‘æ–‡ä»¶
import librosa
audio, sr = librosa.load('data/audio_samples/test.wav', sr=16000)
result = recognizer.recognize(audio)

print(f"è¯†åˆ«ç»“æœ: {result['text']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
```

### 3. å®æ—¶è¯­éŸ³è¯†åˆ«

```bash
python examples/realtime_recognition.py --model models/acoustic_model/wav2vec2.tflite --language zh-cn
```

### 4. å…³é”®è¯æ£€æµ‹

```bash
python examples/keyword_detection.py --keywords "ä½ å¥½" "å°åŠ©æ‰‹" --model models/keyword_spotting/keyword_model.tflite
```

## æ ¸å¿ƒç»„ä»¶

### 1. éŸ³é¢‘å¤„ç†å™¨

```python
import numpy as np
import librosa
import scipy.signal
from typing import Tuple, Optional
import pyaudio
import threading
import queue

class AudioProcessor:
    """éŸ³é¢‘å¤„ç†å™¨"""
    
    def __init__(self, sample_rate: int = 16000, chunk_size: int = 1024):
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        
        # éŸ³é¢‘å‚æ•°
        self.channels = 1
        self.format = pyaudio.paFloat32
        
        # é¢„å¤„ç†å‚æ•°
        self.preemphasis_coeff = 0.97
        self.frame_length = 400  # 25ms at 16kHz
        self.frame_step = 160    # 10ms at 16kHz
        
        # å™ªå£°æŠ‘åˆ¶å‚æ•°
        self.noise_reduction_enabled = True
        self.noise_profile = None
        
        # è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°
        self.vad_threshold = 0.01
        self.min_speech_duration = 0.5  # æœ€å°è¯­éŸ³æŒç»­æ—¶é—´
        self.max_silence_duration = 1.0  # æœ€å¤§é™éŸ³æŒç»­æ—¶é—´
    
    def preprocess_audio(self, audio: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†éŸ³é¢‘ä¿¡å·"""
        # å½’ä¸€åŒ–
        audio = audio / np.max(np.abs(audio))
        
        # é¢„åŠ é‡
        if self.preemphasis_coeff > 0:
            audio = np.append(audio[0], audio[1:] - self.preemphasis_coeff * audio[:-1])
        
        # å™ªå£°æŠ‘åˆ¶
        if self.noise_reduction_enabled and self.noise_profile is not None:
            audio = self.reduce_noise(audio)
        
        return audio
    
    def reduce_noise(self, audio: np.ndarray) -> np.ndarray:
        """å™ªå£°æŠ‘åˆ¶"""
        # ç®€å•çš„è°±å‡æ³•å™ªå£°æŠ‘åˆ¶
        # è®¡ç®—çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
        f, t, stft = scipy.signal.stft(audio, fs=self.sample_rate, 
                                      nperseg=self.frame_length, 
                                      noverlap=self.frame_length - self.frame_step)
        
        # è®¡ç®—åŠŸç‡è°±
        magnitude = np.abs(stft)
        phase = np.angle(stft)
        
        # ä¼°è®¡å™ªå£°åŠŸç‡è°±
        if self.noise_profile is None:
            # ä½¿ç”¨å‰å‡ å¸§ä½œä¸ºå™ªå£°ä¼°è®¡
            noise_frames = min(10, magnitude.shape[1])
            self.noise_profile = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)
        
        # è°±å‡æ³•
        alpha = 2.0  # è¿‡å‡å› å­
        beta = 0.01  # æœ€å°å¢ç›Š
        
        noise_power = self.noise_profile ** 2
        signal_power = magnitude ** 2
        
        # è®¡ç®—å¢ç›Š
        gain = 1 - alpha * (noise_power / signal_power)
        gain = np.maximum(gain, beta)
        
        # åº”ç”¨å¢ç›Š
        enhanced_magnitude = magnitude * gain
        
        # é‡æ„ä¿¡å·
        enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
        _, enhanced_audio = scipy.signal.istft(enhanced_stft, fs=self.sample_rate,
                                              nperseg=self.frame_length,
                                              noverlap=self.frame_length - self.frame_step)
        
        return enhanced_audio
    
    def detect_voice_activity(self, audio: np.ndarray) -> Tuple[bool, float]:
        """è¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
        # è®¡ç®—çŸ­æ—¶èƒ½é‡
        frame_length = int(0.025 * self.sample_rate)  # 25ms
        frame_step = int(0.010 * self.sample_rate)    # 10ms
        
        energy = []
        for i in range(0, len(audio) - frame_length, frame_step):
            frame = audio[i:i + frame_length]
            frame_energy = np.sum(frame ** 2) / len(frame)
            energy.append(frame_energy)
        
        # è®¡ç®—å¹³å‡èƒ½é‡
        avg_energy = np.mean(energy)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºè¯­éŸ³
        is_speech = avg_energy > self.vad_threshold
        
        return is_speech, avg_energy
    
    def extract_mfcc_features(self, audio: np.ndarray, n_mfcc: int = 13) -> np.ndarray:
        """æå–MFCCç‰¹å¾"""
        # è®¡ç®—MFCC
        mfcc = librosa.feature.mfcc(
            y=audio,
            sr=self.sample_rate,
            n_mfcc=n_mfcc,
            n_fft=512,
            hop_length=self.frame_step,
            win_length=self.frame_length
        )
        
        # è®¡ç®—ä¸€é˜¶å’ŒäºŒé˜¶å·®åˆ†
        delta_mfcc = librosa.feature.delta(mfcc)
        delta2_mfcc = librosa.feature.delta(mfcc, order=2)
        
        # åˆå¹¶ç‰¹å¾
        features = np.vstack([mfcc, delta_mfcc, delta2_mfcc])
        
        # è½¬ç½®ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥æ ¼å¼ (time_steps, features)
        features = features.T
        
        return features
    
    def extract_mel_spectrogram(self, audio: np.ndarray, n_mels: int = 80) -> np.ndarray:
        """æå–Melé¢‘è°±å›¾"""
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=self.sample_rate,
            n_mels=n_mels,
            n_fft=512,
            hop_length=self.frame_step,
            win_length=self.frame_length
        )
        
        # è½¬æ¢ä¸ºå¯¹æ•°åˆ»åº¦
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        # è½¬ç½®
        log_mel_spec = log_mel_spec.T
        
        return log_mel_spec

class RealTimeAudioCapture:
    """å®æ—¶éŸ³é¢‘æ•è·"""
    
    def __init__(self, processor: AudioProcessor, callback=None):
        self.processor = processor
        self.callback = callback
        
        # PyAudioè®¾ç½®
        self.pyaudio = pyaudio.PyAudio()
        self.stream = None
        
        # ç¼“å†²åŒº
        self.audio_buffer = queue.Queue()
        self.is_recording = False
        
        # è¯­éŸ³æ£€æµ‹çŠ¶æ€
        self.speech_detected = False
        self.speech_start_time = None
        self.last_speech_time = None
    
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        try:
            self.stream = self.pyaudio.open(
                format=self.processor.format,
                channels=self.processor.channels,
                rate=self.processor.sample_rate,
                input=True,
                frames_per_buffer=self.processor.chunk_size,
                stream_callback=self._audio_callback
            )
            
            self.is_recording = True
            self.stream.start_stream()
            
            print("å¼€å§‹å½•éŸ³...")
            
        except Exception as e:
            print(f"å½•éŸ³å¯åŠ¨å¤±è´¥: {e}")
    
    def stop_recording(self):
        """åœæ­¢å½•éŸ³"""
        self.is_recording = False
        
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        
        self.pyaudio.terminate()
        print("å½•éŸ³å·²åœæ­¢")
    
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°"""
        if status:
            print(f"éŸ³é¢‘çŠ¶æ€: {status}")
        
        # è½¬æ¢éŸ³é¢‘æ•°æ®
        audio_data = np.frombuffer(in_data, dtype=np.float32)
        
        # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
        is_speech, energy = self.processor.detect_voice_activity(audio_data)
        
        current_time = time.time()
        
        if is_speech:
            if not self.speech_detected:
                # è¯­éŸ³å¼€å§‹
                self.speech_detected = True
                self.speech_start_time = current_time
                print("æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹")
            
            self.last_speech_time = current_time
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self.audio_buffer.put(audio_data)
            
        else:
            if self.speech_detected:
                # æ£€æŸ¥æ˜¯å¦è¯­éŸ³ç»“æŸ
                silence_duration = current_time - self.last_speech_time
                
                if silence_duration > self.processor.max_silence_duration:
                    # è¯­éŸ³ç»“æŸ
                    self.speech_detected = False
                    print("æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ")
                    
                    # å¤„ç†ç´¯ç§¯çš„éŸ³é¢‘
                    self._process_accumulated_audio()
        
        return (in_data, pyaudio.paContinue)
    
    def _process_accumulated_audio(self):
        """å¤„ç†ç´¯ç§¯çš„éŸ³é¢‘æ•°æ®"""
        if self.audio_buffer.empty():
            return
        
        # åˆå¹¶éŸ³é¢‘æ•°æ®
        audio_chunks = []
        while not self.audio_buffer.empty():
            audio_chunks.append(self.audio_buffer.get())
        
        if len(audio_chunks) == 0:
            return
        
        # åˆå¹¶éŸ³é¢‘
        full_audio = np.concatenate(audio_chunks)
        
        # æ£€æŸ¥æœ€å°è¯­éŸ³é•¿åº¦
        duration = len(full_audio) / self.processor.sample_rate
        if duration < self.processor.min_speech_duration:
            return
        
        # é¢„å¤„ç†éŸ³é¢‘
        processed_audio = self.processor.preprocess_audio(full_audio)
        
        # è°ƒç”¨å›è°ƒå‡½æ•°
        if self.callback:
            self.callback(processed_audio)
```

### 2. è¯­éŸ³è¯†åˆ«å™¨

```python
import numpy as np
import tensorflow as tf
from typing import Dict, List, Any, Optional
import time

class EdgeSpeechRecognizer:
    """è¾¹ç¼˜è¯­éŸ³è¯†åˆ«å™¨"""
    
    def __init__(self, model_path: str, language: str = 'zh-cn', device: str = 'cpu'):
        self.model_path = model_path
        self.language = language
        self.device = device
        
        # æ¨¡å‹ç»„ä»¶
        self.acoustic_model = None
        self.language_model = None
        self.vocabulary = None
        
        # è¯†åˆ«å‚æ•°
        self.beam_width = 5
        self.max_sequence_length = 1000
        self.blank_token_id = 0
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_recognitions': 0,
            'successful_recognitions': 0,
            'average_processing_time': 0,
            'average_confidence': 0
        }
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        try:
            # åŠ è½½å£°å­¦æ¨¡å‹
            if not self.load_acoustic_model():
                return False
            
            # åŠ è½½è¯­è¨€æ¨¡å‹
            if not self.load_language_model():
                print("è­¦å‘Š: è¯­è¨€æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨è´ªå©ªè§£ç ")
            
            # åŠ è½½è¯æ±‡è¡¨
            if not self.load_vocabulary():
                print("è­¦å‘Š: è¯æ±‡è¡¨åŠ è½½å¤±è´¥")
            
            print("è¯­éŸ³è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def load_acoustic_model(self) -> bool:
        """åŠ è½½å£°å­¦æ¨¡å‹"""
        try:
            if self.model_path.endswith('.tflite'):
                # TensorFlow Liteæ¨¡å‹
                self.acoustic_model = tf.lite.Interpreter(model_path=self.model_path)
                self.acoustic_model.allocate_tensors()
                
                # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
                self.input_details = self.acoustic_model.get_input_details()
                self.output_details = self.acoustic_model.get_output_details()
                
            elif self.model_path.endswith('.onnx'):
                # ONNXæ¨¡å‹
                import onnxruntime as ort
                
                providers = ['CPUExecutionProvider']
                if self.device == 'gpu':
                    providers.insert(0, 'CUDAExecutionProvider')
                
                self.acoustic_model = ort.InferenceSession(self.model_path, providers=providers)
                
            else:
                # æ ‡å‡†TensorFlowæ¨¡å‹
                self.acoustic_model = tf.keras.models.load_model(self.model_path)
            
            return True
            
        except Exception as e:
            print(f"å£°å­¦æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_language_model(self) -> bool:
        """åŠ è½½è¯­è¨€æ¨¡å‹"""
        try:
            # è¿™é‡Œå¯ä»¥åŠ è½½n-gramè¯­è¨€æ¨¡å‹æˆ–ç¥ç»è¯­è¨€æ¨¡å‹
            # ç®€åŒ–å®ç°ï¼Œæš‚æ—¶è·³è¿‡
            return True
            
        except Exception as e:
            print(f"è¯­è¨€æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_vocabulary(self) -> bool:
        """åŠ è½½è¯æ±‡è¡¨"""
        try:
            vocab_file = f'data/vocabulary/{self.language}_vocab.txt'
            
            self.vocabulary = {}
            self.id_to_token = {}
            
            with open(vocab_file, 'r', encoding='utf-8') as f:
                for idx, line in enumerate(f):
                    token = line.strip()
                    self.vocabulary[token] = idx
                    self.id_to_token[idx] = token
            
            return True
            
        except Exception as e:
            print(f"è¯æ±‡è¡¨åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤è¯æ±‡è¡¨
            self.create_default_vocabulary()
            return True
    
    def create_default_vocabulary(self):
        """åˆ›å»ºé»˜è®¤è¯æ±‡è¡¨"""
        # ç®€åŒ–çš„ä¸­æ–‡å­—ç¬¦è¯æ±‡è¡¨
        chars = list("abcdefghijklmnopqrstuvwxyz0123456789 ")
        chars.extend(list("çš„ä¸€æ˜¯åœ¨ä¸äº†æœ‰å’Œäººè¿™ä¸­å¤§ä¸ºä¸Šä¸ªå›½æˆ‘ä»¥è¦ä»–æ—¶æ¥ç”¨ä»¬ç”Ÿåˆ°ä½œåœ°äºå‡ºå°±åˆ†å¯¹æˆä¼šå¯ä¸»å‘å¹´åŠ¨åŒå·¥ä¹Ÿèƒ½ä¸‹è¿‡å­è¯´äº§ç§é¢è€Œæ–¹åå¤šå®šè¡Œå­¦æ³•æ‰€æ°‘å¾—ç»åä¸‰ä¹‹è¿›ç€ç­‰éƒ¨åº¦å®¶ç”µåŠ›é‡Œå¦‚æ°´åŒ–é«˜è‡ªäºŒç†èµ·å°ç‰©ç°å®åŠ é‡éƒ½ä¸¤ä½“åˆ¶æœºå½“ä½¿ç‚¹ä»ä¸šæœ¬å»æŠŠæ€§å¥½åº”å¼€å®ƒåˆè¿˜å› ç”±å…¶äº›ç„¶å‰å¤–å¤©æ”¿å››æ—¥é‚£ç¤¾ä¹‰äº‹å¹³å½¢ç›¸å…¨è¡¨é—´æ ·ä¸å…³å„é‡æ–°çº¿å†…æ•°æ­£å¿ƒåä½ æ˜çœ‹åŸåˆä¹ˆåˆ©æ¯”æˆ–ä½†è´¨æ°”ç¬¬å‘é“å‘½æ­¤å˜æ¡åªæ²¡ç»“è§£é—®æ„å»ºæœˆå…¬æ— ç³»å†›å¾ˆæƒ…è€…æœ€ç«‹ä»£æƒ³å·²é€šå¹¶æç›´é¢˜å…šç¨‹å±•äº”æœæ–™è±¡å‘˜é©ä½å…¥å¸¸æ–‡æ€»æ¬¡å“å¼æ´»è®¾åŠç®¡ç‰¹ä»¶é•¿æ±‚è€å¤´åŸºèµ„è¾¹æµè·¯çº§å°‘å›¾å±±ç»Ÿæ¥çŸ¥è¾ƒå°†ç»„è§è®¡åˆ«å¥¹æ‰‹è§’æœŸæ ¹è®ºè¿å†œæŒ‡å‡ ä¹åŒºå¼ºæ”¾å†³è¥¿è¢«å¹²åšå¿…æˆ˜å…ˆå›åˆ™ä»»å–æ®å¤„é˜Ÿå—ç»™è‰²å…‰é—¨å³ä¿æ²»åŒ—é€ ç™¾è§„çƒ­é¢†ä¸ƒæµ·å£ä¸œå¯¼å™¨å‹å¿—ä¸–é‡‘å¢äº‰æµé˜¶æ²¹æ€æœ¯æäº¤å—è”ä»€è®¤å…­å…±æƒæ”¶è¯æ”¹æ¸…å·±ç¾å†é‡‡è½¬æ›´å•é£åˆ‡æ‰“ç™½æ•™é€ŸèŠ±å¸¦å®‰åœºèº«è½¦ä¾‹çœŸåŠ¡å…·ä¸‡æ¯ç›®è‡³è¾¾èµ°ç§¯ç¤ºè®®å£°æŠ¥æ–—å®Œç±»å…«ç¦»ååç¡®æ‰ç§‘å¼ ä¿¡é©¬èŠ‚è¯ç±³æ•´ç©ºå…ƒå†µä»Šé›†æ¸©ä¼ åœŸè®¸æ­¥ç¾¤å¹¿çŸ³è®°éœ€æ®µç ”ç•Œæ‹‰æ—å¾‹å«ä¸”ç©¶è§‚è¶Šç»‡è£…å½±ç®—ä½æŒéŸ³ä¼—ä¹¦å¸ƒå¤å®¹å„¿é¡»é™…å•†ééªŒè¿æ–­æ·±éš¾è¿‘çŸ¿åƒå‘¨å§”ç´ æŠ€å¤‡åŠåŠé’çœåˆ—ä¹ å“çº¦æ”¯èˆ¬å²æ„ŸåŠ³ä¾¿å›¢å¾€é…¸å†å¸‚å…‹ä½•é™¤æ¶ˆæ„åºœç§°å¤ªå‡†ç²¾å€¼å·ç‡æ—ç»´åˆ’é€‰æ ‡å†™å­˜å€™æ¯›äº²å¿«æ•ˆæ–¯é™¢æŸ¥æ±Ÿå‹çœ¼ç‹æŒ‰æ ¼å…»æ˜“ç½®æ´¾å±‚ç‰‡å§‹å´ä¸“çŠ¶è‚²å‚äº¬è¯†é€‚å±åœ†åŒ…ç«ä½è°ƒæ»¡å¿å±€ç…§å‚çº¢ç»†å¼•å¬è¯¥é“ä»·ä¸¥")
        
        self.vocabulary = {'<blank>': 0}
        self.id_to_token = {0: '<blank>'}
        
        for idx, char in enumerate(chars, 1):
            self.vocabulary[char] = idx
            self.id_to_token[idx] = char
    
    def recognize(self, audio: np.ndarray) -> Dict[str, Any]:
        """è¯†åˆ«è¯­éŸ³"""
        start_time = time.time()
        
        try:
            # æå–ç‰¹å¾
            features = self.extract_features(audio)
            
            # å£°å­¦æ¨¡å‹æ¨ç†
            logits = self.acoustic_inference(features)
            
            # è§£ç 
            text, confidence = self.decode_logits(logits)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (time.time() - start_time) * 1000
            
            # æ›´æ–°ç»Ÿè®¡
            self.update_stats(processing_time, confidence, len(text) > 0)
            
            result = {
                'text': text,
                'confidence': confidence,
                'processing_time_ms': processing_time,
                'audio_duration_s': len(audio) / 16000,
                'features_shape': features.shape
            }
            
            return result
            
        except Exception as e:
            print(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return {
                'text': '',
                'confidence': 0.0,
                'processing_time_ms': (time.time() - start_time) * 1000,
                'error': str(e)
            }
    
    def extract_features(self, audio: np.ndarray) -> np.ndarray:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        # ä½¿ç”¨AudioProcessoræå–ç‰¹å¾
        processor = AudioProcessor()
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç‰¹å¾
        if 'wav2vec' in self.model_path.lower():
            # Wav2Vec2ä½¿ç”¨åŸå§‹éŸ³é¢‘
            # è°ƒæ•´é•¿åº¦åˆ°å›ºå®šå¤§å°
            target_length = 16000 * 10  # 10ç§’
            if len(audio) > target_length:
                audio = audio[:target_length]
            else:
                audio = np.pad(audio, (0, target_length - len(audio)), 'constant')
            
            features = audio.reshape(1, -1)  # (batch, time)
            
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨MFCCæˆ–Melé¢‘è°±å›¾
            features = processor.extract_mfcc_features(audio)
            features = np.expand_dims(features, axis=0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        
        return features
    
    def acoustic_inference(self, features: np.ndarray) -> np.ndarray:
        """å£°å­¦æ¨¡å‹æ¨ç†"""
        if isinstance(self.acoustic_model, tf.lite.Interpreter):
            # TensorFlow Liteæ¨ç†
            self.acoustic_model.set_tensor(self.input_details[0]['index'], features.astype(np.float32))
            self.acoustic_model.invoke()
            logits = self.acoustic_model.get_tensor(self.output_details[0]['index'])
            
        elif hasattr(self.acoustic_model, 'run'):
            # ONNXæ¨ç†
            input_name = self.acoustic_model.get_inputs()[0].name
            logits = self.acoustic_model.run(None, {input_name: features.astype(np.float32)})[0]
            
        else:
            # TensorFlowæ¨ç†
            logits = self.acoustic_model.predict(features)
        
        return logits
    
    def decode_logits(self, logits: np.ndarray) -> tuple[str, float]:
        """è§£ç logitsä¸ºæ–‡æœ¬"""
        # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
        if len(logits.shape) == 3:
            logits = logits[0]
        
        # è´ªå©ªè§£ç 
        predicted_ids = np.argmax(logits, axis=-1)
        
        # CTCè§£ç  - ç§»é™¤é‡å¤å’Œç©ºç™½æ ‡è®°
        decoded_ids = []
        prev_id = -1
        
        for pred_id in predicted_ids:
            if pred_id != prev_id and pred_id != self.blank_token_id:
                decoded_ids.append(pred_id)
            prev_id = pred_id
        
        # è½¬æ¢ä¸ºæ–‡æœ¬
        text = ""
        confidences = []
        
        for token_id in decoded_ids:
            if token_id in self.id_to_token:
                text += self.id_to_token[token_id]
                # è®¡ç®—è¯¥tokençš„ç½®ä¿¡åº¦
                token_confidence = np.max(tf.nn.softmax(logits[predicted_ids == token_id]))
                confidences.append(token_confidence)
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = np.mean(confidences) if confidences else 0.0
        
        return text, float(avg_confidence)
    
    def beam_search_decode(self, logits: np.ndarray) -> tuple[str, float]:
        """æŸæœç´¢è§£ç """
        # ç®€åŒ–çš„æŸæœç´¢å®ç°
        vocab_size = logits.shape[-1]
        seq_length = logits.shape[0]
        
        # åˆå§‹åŒ–æŸ
        beams = [{'sequence': [], 'score': 0.0}]
        
        for t in range(seq_length):
            new_beams = []
            
            for beam in beams:
                # è·å–å½“å‰æ—¶é—´æ­¥çš„æ¦‚ç‡
                probs = tf.nn.softmax(logits[t]).numpy()
                
                # é€‰æ‹©top-kå€™é€‰
                top_k_indices = np.argsort(probs)[-self.beam_width:]
                
                for idx in top_k_indices:
                    new_sequence = beam['sequence'] + [idx]
                    new_score = beam['score'] + np.log(probs[idx])
                    
                    new_beams.append({
                        'sequence': new_sequence,
                        'score': new_score
                    })
            
            # ä¿ç•™æœ€ä½³çš„beam_widthä¸ªå€™é€‰
            new_beams.sort(key=lambda x: x['score'], reverse=True)
            beams = new_beams[:self.beam_width]
        
        # é€‰æ‹©æœ€ä½³åºåˆ—
        best_beam = beams[0]
        
        # CTCåå¤„ç†
        decoded_sequence = self.ctc_postprocess(best_beam['sequence'])
        
        # è½¬æ¢ä¸ºæ–‡æœ¬
        text = ''.join([self.id_to_token.get(idx, '') for idx in decoded_sequence])
        confidence = np.exp(best_beam['score'] / len(decoded_sequence))
        
        return text, confidence
    
    def ctc_postprocess(self, sequence: List[int]) -> List[int]:
        """CTCåå¤„ç†"""
        decoded = []
        prev_token = -1
        
        for token in sequence:
            if token != prev_token and token != self.blank_token_id:
                decoded.append(token)
            prev_token = token
        
        return decoded
    
    def update_stats(self, processing_time: float, confidence: float, success: bool):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_recognitions'] += 1
        
        if success:
            self.stats['successful_recognitions'] += 1
        
        # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        total = self.stats['total_recognitions']
        self.stats['average_processing_time'] = (
            (self.stats['average_processing_time'] * (total - 1) + processing_time) / total
        )
        
        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        if success:
            successful = self.stats['successful_recognitions']
            self.stats['average_confidence'] = (
                (self.stats['average_confidence'] * (successful - 1) + confidence) / successful
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = 0.0
        if self.stats['total_recognitions'] > 0:
            success_rate = self.stats['successful_recognitions'] / self.stats['total_recognitions']
        
        return {
            'total_recognitions': self.stats['total_recognitions'],
            'successful_recognitions': self.stats['successful_recognitions'],
            'success_rate': success_rate,
            'average_processing_time_ms': self.stats['average_processing_time'],
            'average_confidence': self.stats['average_confidence'],
            'vocabulary_size': len(self.vocabulary),
            'language': self.language,
            'model_path': self.model_path
        }
```

### 3. å…³é”®è¯æ£€æµ‹å™¨

```python
import numpy as np
import tensorflow as tf
from typing import List, Dict, Any, Optional
import time
from collections import deque

class KeywordSpotter:
    """å…³é”®è¯æ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str, keywords: List[str], device: str = 'cpu'):
        self.model_path = model_path
        self.keywords = keywords
        self.device = device
        
        # æ¨¡å‹
        self.model = None
        self.is_loaded = False
        
        # æ£€æµ‹å‚æ•°
        self.detection_threshold = 0.7
        self.window_size = 1.0  # 1ç§’çª—å£
        self.hop_size = 0.1     # 100msè·³è·ƒ
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = deque(maxlen=int(16000 * self.window_size))
        
        # æ£€æµ‹å†å²
        self.detection_history = deque(maxlen=10)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_detections': 0,
            'keyword_counts': {kw: 0 for kw in keywords},
            'false_positives': 0,
            'average_confidence': 0
        }
    
    def load_model(self) -> bool:
        """åŠ è½½å…³é”®è¯æ£€æµ‹æ¨¡å‹"""
        try:
            if self.model_path.endswith('.tflite'):
                self.model = tf.lite.Interpreter(model_path=self.model_path)
                self.model.allocate_tensors()
                
                self.input_details = self.model.get_input_details()
                self.output_details = self.model.get_output_details()
                
            else:
                self.model = tf.keras.models.load_model(self.model_path)
            
            self.is_loaded = True
            print(f"å…³é”®è¯æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ: {len(self.keywords)} ä¸ªå…³é”®è¯")
            return True
            
        except Exception as e:
            print(f"å…³é”®è¯æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def add_audio(self, audio_chunk: np.ndarray) -> Optional[Dict[str, Any]]:
        """æ·»åŠ éŸ³é¢‘å—å¹¶æ£€æµ‹å…³é”®è¯"""
        if not self.is_loaded:
            return None
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.extend(audio_chunk)
        
        # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦è¶³å¤Ÿ
        if len(self.audio_buffer) < int(16000 * self.window_size):
            return None
        
        # æå–å½“å‰çª—å£çš„éŸ³é¢‘
        window_audio = np.array(list(self.audio_buffer))
        
        # æ‰§è¡Œå…³é”®è¯æ£€æµ‹
        detection_result = self.detect_keywords(window_audio)
        
        return detection_result
    
    def detect_keywords(self, audio: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹å…³é”®è¯"""
        try:
            # æå–ç‰¹å¾
            features = self.extract_features(audio)
            
            # æ¨¡å‹æ¨ç†
            start_time = time.time()
            predictions = self.predict(features)
            inference_time = (time.time() - start_time) * 1000
            
            # è§£æç»“æœ
            detected_keywords = []
            max_confidence = 0.0
            
            for i, keyword in enumerate(self.keywords):
                confidence = predictions[i] if i < len(predictions) else 0.0
                
                if confidence > self.detection_threshold:
                    detected_keywords.append({
                        'keyword': keyword,
                        'confidence': float(confidence),
                        'timestamp': time.time()
                    })
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats['keyword_counts'][keyword] += 1
                    max_confidence = max(max_confidence, confidence)
            
            # æ›´æ–°æ€»ä½“ç»Ÿè®¡
            if detected_keywords:
                self.stats['total_detections'] += 1
                
                # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
                total = self.stats['total_detections']
                self.stats['average_confidence'] = (
                    (self.stats['average_confidence'] * (total - 1) + max_confidence) / total
                )
            
            # è®°å½•æ£€æµ‹å†å²
            self.detection_history.append({
                'timestamp': time.time(),
                'keywords': detected_keywords,
                'max_confidence': max_confidence,
                'inference_time_ms': inference_time
            })
            
            result = {
                'detected_keywords': detected_keywords,
                'max_confidence': max_confidence,
                'inference_time_ms': inference_time,
                'audio_duration_s': len(audio) / 16000
            }
            
            return result
            
        except Exception as e:
            print(f"å…³é”®è¯æ£€æµ‹å¤±è´¥: {e}")
            return {
                'detected_keywords': [],
                'max_confidence': 0.0,
                'inference_time_ms': 0.0,
                'error': str(e)
            }
    
    def extract_features(self, audio: np.ndarray) -> np.ndarray:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        # ä½¿ç”¨MFCCç‰¹å¾
        processor = AudioProcessor()
        features = processor.extract_mfcc_features(audio, n_mfcc=13)
        
        # è°ƒæ•´åˆ°å›ºå®šé•¿åº¦
        target_length = 100  # 100å¸§
        if features.shape[0] > target_length:
            features = features[:target_length]
        else:
            # å¡«å……
            padding = target_length - features.shape[0]
            features = np.pad(features, ((0, padding), (0, 0)), 'constant')
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        features = np.expand_dims(features, axis=0)
        
        return features
    
    def predict(self, features: np.ndarray) -> np.ndarray:
        """æ¨¡å‹é¢„æµ‹"""
        if isinstance(self.model, tf.lite.Interpreter):
            # TensorFlow Liteæ¨ç†
            self.model.set_tensor(self.input_details[0]['index'], features.astype(np.float32))
            self.model.invoke()
            predictions = self.model.get_tensor(self.output_details[0]['index'])
            
        else:
            # TensorFlowæ¨ç†
            predictions = self.model.predict(features)
        
        # åº”ç”¨sigmoidæ¿€æ´»
        predictions = tf.nn.sigmoid(predictions).numpy()
        
        return predictions[0]  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
    
    def reset_buffer(self):
        """é‡ç½®éŸ³é¢‘ç¼“å†²åŒº"""
        self.audio_buffer.clear()
    
    def get_detection_history(self, last_n: int = 10) -> List[Dict[str, Any]]:
        """è·å–æ£€æµ‹å†å²"""
        return list(self.detection_history)[-last_n:]
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_detections': self.stats['total_detections'],
            'keyword_counts': self.stats['keyword_counts'].copy(),
            'false_positives': self.stats['false_positives'],
            'average_confidence': self.stats['average_confidence'],
            'detection_threshold': self.detection_threshold,
            'keywords': self.keywords.copy()
        }
    
    def update_threshold(self, new_threshold: float):
        """æ›´æ–°æ£€æµ‹é˜ˆå€¼"""
        self.detection_threshold = max(0.0, min(1.0, new_threshold))
        print(f"æ£€æµ‹é˜ˆå€¼æ›´æ–°ä¸º: {self.detection_threshold}")
    
    def add_keyword(self, keyword: str):
        """æ·»åŠ æ–°å…³é”®è¯"""
        if keyword not in self.keywords:
            self.keywords.append(keyword)
            self.stats['keyword_counts'][keyword] = 0
            print(f"æ·»åŠ æ–°å…³é”®è¯: {keyword}")
    
    def remove_keyword(self, keyword: str):
        """ç§»é™¤å…³é”®è¯"""
        if keyword in self.keywords:
            self.keywords.remove(keyword)
            del self.stats['keyword_counts'][keyword]
            print(f"ç§»é™¤å…³é”®è¯: {keyword}")
```

## å®æ—¶è¯­éŸ³è¯†åˆ«ç¤ºä¾‹

```python
import argparse
import time
import threading
from src.audio_processor import AudioProcessor, RealTimeAudioCapture
from src.speech_recognizer import EdgeSpeechRecognizer
from src.keyword_spotter import KeywordSpotter

class RealTimeSpeechRecognition:
    """å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ"""
    
    def __init__(self, recognizer_model: str, language: str = 'zh-cn', 
                 keyword_model: str = None, keywords: List[str] = None):
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.audio_processor = AudioProcessor()
        self.speech_recognizer = EdgeSpeechRecognizer(recognizer_model, language)
        
        # å…³é”®è¯æ£€æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
        self.keyword_spotter = None
        if keyword_model and keywords:
            self.keyword_spotter = KeywordSpotter(keyword_model, keywords)
        
        # éŸ³é¢‘æ•è·
        self.audio_capture = RealTimeAudioCapture(
            self.audio_processor, 
            callback=self.process_audio
        )
        
        # ç³»ç»ŸçŠ¶æ€
        self.is_running = False
        self.recognition_mode = 'continuous'  # 'continuous' æˆ– 'keyword_triggered'
        self.keyword_detected = False
        
        # ç»“æœç¼“å­˜
        self.recent_results = []
        self.max_results = 10
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        try:
            # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨
            if not self.speech_recognizer.initialize():
                return False
            
            # åˆå§‹åŒ–å…³é”®è¯æ£€æµ‹å™¨
            if self.keyword_spotter:
                if not self.keyword_spotter.load_model():
                    print("å…³é”®è¯æ£€æµ‹å™¨åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨è¿ç»­è¯†åˆ«æ¨¡å¼")
                    self.keyword_spotter = None
                else:
                    self.recognition_mode = 'keyword_triggered'
            
            print("å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def start(self):
        """å¯åŠ¨å®æ—¶è¯†åˆ«"""
        if not self.initialize():
            return
        
        self.is_running = True
        
        print("å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«...")
        print(f"è¯†åˆ«æ¨¡å¼: {self.recognition_mode}")
        print("è¯´è¯å¼€å§‹è¯†åˆ«ï¼ŒæŒ‰Ctrl+Cé€€å‡º")
        
        # å¯åŠ¨éŸ³é¢‘æ•è·
        self.audio_capture.start_recording()
        
        try:
            # ä¸»å¾ªç¯
            while self.is_running:
                time.sleep(0.1)
                
                # æ˜¾ç¤ºæœ€è¿‘çš„è¯†åˆ«ç»“æœ
                self.display_recent_results()
                
        except KeyboardInterrupt:
            print("\næ­£åœ¨åœæ­¢...")
        
        finally:
            self.stop()
    
    def stop(self):
        """åœæ­¢è¯†åˆ«"""
        self.is_running = False
        self.audio_capture.stop_recording()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics()
        
        print("å®æ—¶è¯­éŸ³è¯†åˆ«å·²åœæ­¢")
    
    def process_audio(self, audio: np.ndarray):
        """å¤„ç†éŸ³é¢‘æ•°æ®"""
        if not self.is_running:
            return
        
        try:
            if self.recognition_mode == 'keyword_triggered' and self.keyword_spotter:
                # å…³é”®è¯è§¦å‘æ¨¡å¼
                self.process_keyword_triggered(audio)
            else:
                # è¿ç»­è¯†åˆ«æ¨¡å¼
                self.process_continuous(audio)
                
        except Exception as e:
            print(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
    
    def process_keyword_triggered(self, audio: np.ndarray):
        """å…³é”®è¯è§¦å‘æ¨¡å¼å¤„ç†"""
        # æ£€æµ‹å…³é”®è¯
        keyword_result = self.keyword_spotter.add_audio(audio)
        
        if keyword_result and keyword_result['detected_keywords']:
            # æ£€æµ‹åˆ°å…³é”®è¯
            detected_kw = keyword_result['detected_keywords'][0]
            print(f"\nğŸ¯ æ£€æµ‹åˆ°å…³é”®è¯: {detected_kw['keyword']} (ç½®ä¿¡åº¦: {detected_kw['confidence']:.3f})")
            
            self.keyword_detected = True
            
            # ç­‰å¾…åç»­è¯­éŸ³è¿›è¡Œè¯†åˆ«
            threading.Timer(0.5, self.trigger_recognition, args=[audio]).start()
    
    def process_continuous(self, audio: np.ndarray):
        """è¿ç»­è¯†åˆ«æ¨¡å¼å¤„ç†"""
        # ç›´æ¥è¿›è¡Œè¯­éŸ³è¯†åˆ«
        self.perform_recognition(audio)
    
    def trigger_recognition(self, audio: np.ndarray):
        """è§¦å‘è¯­éŸ³è¯†åˆ«"""
        if self.keyword_detected:
            self.perform_recognition(audio)
            self.keyword_detected = False
    
    def perform_recognition(self, audio: np.ndarray):
        """æ‰§è¡Œè¯­éŸ³è¯†åˆ«"""
        try:
            # è¯­éŸ³è¯†åˆ«
            result = self.speech_recognizer.recognize(audio)
            
            if result['text'].strip():
                # æ·»åŠ æ—¶é—´æˆ³
                result['timestamp'] = time.time()
                result['formatted_time'] = time.strftime('%H:%M:%S')
                
                # æ·»åŠ åˆ°ç»“æœç¼“å­˜
                self.recent_results.append(result)
                if len(self.recent_results) > self.max_results:
                    self.recent_results.pop(0)
                
                # å®æ—¶æ˜¾ç¤ºç»“æœ
                print(f"\nğŸ¤ [{result['formatted_time']}] è¯†åˆ«ç»“æœ: {result['text']}")
                print(f"   ç½®ä¿¡åº¦: {result['confidence']:.3f}, å¤„ç†æ—¶é—´: {result['processing_time_ms']:.1f}ms")
                
        except Exception as e:
            print(f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
    
    def display_recent_results(self):
        """æ˜¾ç¤ºæœ€è¿‘çš„è¯†åˆ«ç»“æœ"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ˜¾ç¤ºé€»è¾‘
        pass
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n=== è¯†åˆ«ç»Ÿè®¡ ===")
        
        # è¯­éŸ³è¯†åˆ«ç»Ÿè®¡
        speech_stats = self.speech_recognizer.get_stats()
        print(f"æ€»è¯†åˆ«æ¬¡æ•°: {speech_stats['total_recognitions']}")
        print(f"æˆåŠŸè¯†åˆ«æ¬¡æ•°: {speech_stats['successful_recognitions']}")
        print(f"è¯†åˆ«æˆåŠŸç‡: {speech_stats['success_rate']:.1%}")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {speech_stats['average_processing_time_ms']:.1f}ms")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {speech_stats['average_confidence']:.3f}")
        
        # å…³é”®è¯æ£€æµ‹ç»Ÿè®¡
        if self.keyword_spotter:
            keyword_stats = self.keyword_spotter.get_stats()
            print(f"\nå…³é”®è¯æ£€æµ‹æ¬¡æ•°: {keyword_stats['total_detections']}")
            print("å„å…³é”®è¯æ£€æµ‹æ¬¡æ•°:")
            for kw, count in keyword_stats['keyword_counts'].items():
                print(f"  {kw}: {count}")
        
        # æœ€è¿‘è¯†åˆ«ç»“æœ
        if self.recent_results:
            print(f"\næœ€è¿‘ {len(self.recent_results)} æ¬¡è¯†åˆ«ç»“æœ:")
            for i, result in enumerate(self.recent_results[-5:], 1):
                print(f"  {i}. [{result['formatted_time']}] {result['text']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")

def main():
    parser = argparse.ArgumentParser(description='å®æ—¶è¯­éŸ³è¯†åˆ«')
    parser.add_argument('--model', required=True, help='è¯­éŸ³è¯†åˆ«æ¨¡å‹è·¯å¾„')
    parser.add_argument('--language', default='zh-cn', help='è¯†åˆ«è¯­è¨€')
    parser.add_argument('--keyword_model', help='å…³é”®è¯æ£€æµ‹æ¨¡å‹è·¯å¾„')
    parser.add_argument('--keywords', nargs='+', help='å…³é”®è¯åˆ—è¡¨')
    parser.add_argument('--mode', choices=['continuous', 'keyword_triggered'], 
                       default='continuous', help='è¯†åˆ«æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ
    recognition_system = RealTimeSpeechRecognition(
        args.model, args.language, args.keyword_model, args.keywords
    )
    
    # è®¾ç½®è¯†åˆ«æ¨¡å¼
    if args.mode == 'keyword_triggered' and not (args.keyword_model and args.keywords):
        print("å…³é”®è¯è§¦å‘æ¨¡å¼éœ€è¦æä¾›å…³é”®è¯æ¨¡å‹å’Œå…³é”®è¯åˆ—è¡¨")
        return
    
    recognition_system.recognition_mode = args.mode
    
    # å¯åŠ¨è¯†åˆ«
    recognition_system.start()

if __name__ == "__main__":
    main()
```

## åº”ç”¨åœºæ™¯

- **æ™ºèƒ½éŸ³ç®±**: è¯­éŸ³æ§åˆ¶å’ŒæŸ¥è¯¢
- **è½¦è½½ç³»ç»Ÿ**: å…æè¯­éŸ³æ“ä½œ
- **æ™ºèƒ½å®¶å±…**: è¯­éŸ³æ§åˆ¶å®¶ç”µè®¾å¤‡
- **ä¼šè®®è®°å½•**: å®æ—¶è¯­éŸ³è½¬æ–‡å­—
- **è¾…åŠ©è¾“å…¥**: è¯­éŸ³è¾“å…¥æ³•å’Œå¬å†™
- **å®‰é˜²ç›‘æ§**: è¯­éŸ³äº‹ä»¶æ£€æµ‹

## æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹ä¼˜åŒ–
- **æ¨¡å‹é‡åŒ–**: ä½¿ç”¨INT8é‡åŒ–å‡å°‘æ¨¡å‹å¤§å°
- **æ¨¡å‹å‰ªæ**: ç§»é™¤ä¸é‡è¦çš„å‚æ•°
- **çŸ¥è¯†è’¸é¦**: ç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹

### 2. éŸ³é¢‘å¤„ç†ä¼˜åŒ–
- **å®æ—¶å¤„ç†**: æµå¼éŸ³é¢‘å¤„ç†
- **å™ªå£°æŠ‘åˆ¶**: æé«˜è¯†åˆ«å‡†ç¡®ç‡
- **å›å£°æ¶ˆé™¤**: æ”¹å–„éŸ³é¢‘è´¨é‡

### 3. ç³»ç»Ÿä¼˜åŒ–
- **ç¼“å­˜æœºåˆ¶**: ç¼“å­˜å¸¸ç”¨è¯æ±‡å’Œæ¨¡å‹
- **å¹¶è¡Œå¤„ç†**: å¤šçº¿ç¨‹éŸ³é¢‘å¤„ç†
- **å†…å­˜ç®¡ç†**: ä¼˜åŒ–å†…å­˜ä½¿ç”¨

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•æé«˜è¯†åˆ«å‡†ç¡®ç‡ï¼Ÿ
A: 
1. ä½¿ç”¨é«˜è´¨é‡çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡
2. åœ¨å®‰é™ç¯å¢ƒä¸­è¿›è¡Œè¯†åˆ«
3. è°ƒæ•´è¯†åˆ«å‚æ•°å’Œé˜ˆå€¼
4. ä½¿ç”¨é¢†åŸŸç‰¹å®šçš„è¯­è¨€æ¨¡å‹

### Q: å¦‚ä½•å¤„ç†æ–¹è¨€å’Œå£éŸ³ï¼Ÿ
A: 
1. ä½¿ç”¨å¤šæ–¹è¨€è®­ç»ƒçš„æ¨¡å‹
2. æ”¶é›†ç‰¹å®šæ–¹è¨€çš„è®­ç»ƒæ•°æ®
3. è¿›è¡Œæ¨¡å‹å¾®è°ƒå’Œé€‚åº”

### Q: å¦‚ä½•é™ä½å»¶è¿Ÿï¼Ÿ
A: 
1. ä½¿ç”¨æ›´è½»é‡çº§çš„æ¨¡å‹
2. ä¼˜åŒ–éŸ³é¢‘é¢„å¤„ç†æµç¨‹
3. ä½¿ç”¨ç¡¬ä»¶åŠ é€Ÿ
4. å‡å°‘ç½‘ç»œä¼ è¾“

## æ‰©å±•åŠŸèƒ½

- **å¤šè¯­è¨€è¯†åˆ«**: æ”¯æŒå¤šç§è¯­è¨€æ··åˆè¯†åˆ«
- **è¯´è¯äººè¯†åˆ«**: è¯†åˆ«ä¸åŒè¯´è¯äºº
- **æƒ…æ„Ÿè¯†åˆ«**: åˆ†æè¯­éŸ³ä¸­çš„æƒ…æ„Ÿ
- **è¯­éŸ³åˆæˆ**: æ–‡å­—è½¬è¯­éŸ³åŠŸèƒ½
- **å¯¹è¯ç®¡ç†**: å¤šè½®å¯¹è¯ç†è§£

æ¬¢è¿è´¡çŒ®ä»£ç å’Œæå‡ºæ”¹è¿›å»ºè®®ï¼